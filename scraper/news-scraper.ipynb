{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('deeplearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c64c80a847a2bc1167ae9f3d93f268f3978450d0b767fc4cbce34d6ba3b096a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Scraper\n",
    "\n",
    "This scraper is built for this project that will determine whether the stock market prices will be on a down or uptrend based on news and stock price data. I still haven't determined whether I should include the price prediction. That depends well on the data I will be able to get after I have preprocessed the text from the news sources that I have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n  %reload_ext nb_black\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"%load_ext nb_black\";\n                var nbb_formatted_code = \"%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"from IPython.core.display import clear_output\\nfrom datetime import timedelta\\nfrom bs4 import BeautifulSoup\\nfrom time import time, sleep\\nfrom random import randint\\nfrom warnings import warn\\nfrom pytz import timezone\\n\\nimport pandas as pd\\nimport numpy as np\\nimport threading\\nimport grequests\\nimport warnings\\nimport datetime\\nimport requests\\nimport math\\nimport json\\nimport csv\\nimport os\\nimport io\\nimport re\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_formatted_code = \"from IPython.core.display import clear_output\\nfrom datetime import timedelta\\nfrom bs4 import BeautifulSoup\\nfrom time import time, sleep\\nfrom random import randint\\nfrom warnings import warn\\nfrom pytz import timezone\\n\\nimport pandas as pd\\nimport numpy as np\\nimport threading\\nimport grequests\\nimport warnings\\nimport datetime\\nimport requests\\nimport math\\nimport json\\nimport csv\\nimport os\\nimport io\\nimport re\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.core.display import clear_output\n",
    "from datetime import timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time, sleep\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import grequests\n",
    "import warnings\n",
    "import datetime\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "### Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 24;\n                var nbb_unformatted_code = \"dfx = \\\"https://www.dailyfx.com/market-news/articles/\\\"\\nfxe = \\\"https://www.fxempire.com/indices/spx500-usd/news?page=\\\"\\ncnbc = \\\"https://www.cnbc.com/sp-500/?page=\\\"\\ninv = \\\"https://www.investing.com/indices/us-spx-500-news/\\\"\\n\\nsrcs = [dfx, fxe, cnbc, inv]\";\n                var nbb_formatted_code = \"dfx = \\\"https://www.dailyfx.com/market-news/articles/\\\"\\nfxe = \\\"https://www.fxempire.com/indices/spx500-usd/news?page=\\\"\\ncnbc = \\\"https://www.cnbc.com/sp-500/?page=\\\"\\ninv = \\\"https://www.investing.com/indices/us-spx-500-news/\\\"\\n\\nsrcs = [dfx, fxe, cnbc, inv]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "dfx = \"https://www.dailyfx.com/market-news/articles/\"\n",
    "fxe = \"https://www.fxempire.com/indices/spx500-usd/news?page=\"\n",
    "cnbc = \"https://www.cnbc.com/sp-500/?page=\"\n",
    "inv = \"https://www.investing.com/indices/us-spx-500-news/\"\n",
    "\n",
    "srcs = [dfx, fxe, cnbc, inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# checking the URLs of the news sites for a positive response\\nheaders = {\\n    \\\"User-Agent\\\":\\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\\\"\\n}\\n\\nrs = (grequests.get(u+\\\"1\\\", headers=headers) for u in srcs)\\ngrequests.map(rs)\";\n                var nbb_formatted_code = \"# checking the URLs of the news sites for a positive response\\nheaders = {\\n    \\\"User-Agent\\\": \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\\\"\\n}\\n\\nrs = (grequests.get(u + \\\"1\\\", headers=headers) for u in srcs)\\ngrequests.map(rs)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# checking the URLs of the news sites for a positive response\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "}\n",
    "\n",
    "rs = (grequests.get(u+\"1\", headers=headers) for u in srcs)\n",
    "grequests.map(rs)"
   ]
  },
  {
   "source": [
    "Responses are good after setting a user agent. Might need to have a rotation of IP addresses since I intend to get around 50 pages' worth of headlines per source. Will resort to single thread pulls to avoid being tracked."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"# for DailyFX\\ndef get_dailyfx(page):\\n    url = dfx + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if(response.ok):\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"body > div.dfx-slidableContent > div > div.container > div > div.col-xl-8.dfx-border--r-xl-1 > div.dfx-articleList.jsdfx-articleList\\\")\\n\\n        for article in articles:  \\n            headlines = article.select(\\\"span.align-middle\\\")\\n            span = article.select(\\\"span.text-nowrap\\\")\\n            \\n        for i in range(0, len(headlines)):\\n            headline = str(headlines[i]).split('>')[1].split('<')[0]\\n            date = span[i]['data-time'].split('+')[0]\\n            dt = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\\n            dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n            nres = {\\n                \\\"headline\\\" : headline,\\n                \\\"date\\\" : dt_\\n            }\\n            collection.append(nres)\\n            #sleep(2)\\n\\n        #print(collection)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n\\n    return collection\";\n                var nbb_formatted_code = \"# for DailyFX\\ndef get_dailyfx(page):\\n    url = dfx + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if response.ok:\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\n            \\\"body > div.dfx-slidableContent > div > div.container > div > div.col-xl-8.dfx-border--r-xl-1 > div.dfx-articleList.jsdfx-articleList\\\"\\n        )\\n\\n        for article in articles:\\n            headlines = article.select(\\\"span.align-middle\\\")\\n            span = article.select(\\\"span.text-nowrap\\\")\\n\\n        for i in range(0, len(headlines)):\\n            headline = str(headlines[i]).split(\\\">\\\")[1].split(\\\"<\\\")[0]\\n            date = span[i][\\\"data-time\\\"].split(\\\"+\\\")[0]\\n            dt = datetime.datetime.strptime(date, \\\"%Y-%m-%dT%H:%M:%S\\\") + timedelta(\\n                hours=-5\\n            )  # adjusting for EST\\n            dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n            nres = {\\\"headline\\\": headline, \\\"date\\\": dt_}\\n            collection.append(nres)\\n            # sleep(2)\\n\\n        # print(collection)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n\\n    return collection\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# for DailyFX\n",
    "def get_dailyfx(page):\n",
    "    url = dfx + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"body > div.dfx-slidableContent > div > div.container > div > div.col-xl-8.dfx-border--r-xl-1 > div.dfx-articleList.jsdfx-articleList\")\n",
    "\n",
    "        for article in articles:  \n",
    "            headlines = article.select(\"span.align-middle\")\n",
    "            span = article.select(\"span.text-nowrap\")\n",
    "            \n",
    "        for i in range(0, len(headlines)):\n",
    "            headline = str(headlines[i]).split('>')[1].split('<')[0]\n",
    "            date = span[i]['data-time'].split('+')[0]\n",
    "            dt = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\n",
    "            dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")\n",
    "\n",
    "            nres = {\n",
    "                \"headline\" : headline,\n",
    "                \"date\" : dt_\n",
    "            }\n",
    "            collection.append(nres)\n",
    "            #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"# for FXEmpire\\ndef get_fxempire(page):\\n    url = fxe + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if(response.ok):\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"div.Article-sc-178sudu-0\\\")\\n        \\n        for article in articles:\\n            text = article.get_text()\\n            text_ = text.split('\\\\n')\\n            i = 1\\n\\n            for title in article.select(\\\"a.Link-y81klt-0\\\"):\\n                preproc = title.get_text()\\n                preproc_ = preproc.split('\\\\n')[0]\\n                i +=1\\n\\n                if i==2:\\n                    #headline.append(preproc_)\\n                    hd = preproc_\\n\\n            for time in article.select(\\\"time\\\"):\\n                dt = time[\\\"datetime\\\"]\\n                dt = datetime.datetime.strptime(dt, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\\n                dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")           \\n\\n            nres = {\\n                \\\"headline\\\" : hd,\\n                \\\"date\\\" : dt_\\n            }\\n            collection.append(nres)\\n            #sleep(2)\\n\\n        #print(collection)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n    \\n    return collection\";\n                var nbb_formatted_code = \"# for FXEmpire\\ndef get_fxempire(page):\\n    url = fxe + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if response.ok:\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"div.Article-sc-178sudu-0\\\")\\n\\n        for article in articles:\\n            text = article.get_text()\\n            text_ = text.split(\\\"\\\\n\\\")\\n            i = 1\\n\\n            for title in article.select(\\\"a.Link-y81klt-0\\\"):\\n                preproc = title.get_text()\\n                preproc_ = preproc.split(\\\"\\\\n\\\")[0]\\n                i += 1\\n\\n                if i == 2:\\n                    # headline.append(preproc_)\\n                    hd = preproc_\\n\\n            for time in article.select(\\\"time\\\"):\\n                dt = time[\\\"datetime\\\"]\\n                dt = datetime.datetime.strptime(dt, \\\"%Y-%m-%dT%H:%M:%S\\\") + timedelta(\\n                    hours=-5\\n                )  # adjusting for EST\\n                dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n            nres = {\\\"headline\\\": hd, \\\"date\\\": dt_}\\n            collection.append(nres)\\n            # sleep(2)\\n\\n        # print(collection)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n\\n    return collection\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# for FXEmpire\n",
    "def get_fxempire(page):\n",
    "    url = fxe + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"div.Article-sc-178sudu-0\")\n",
    "        \n",
    "        for article in articles:\n",
    "            text = article.get_text()\n",
    "            text_ = text.split('\\n')\n",
    "            i = 1\n",
    "\n",
    "            for title in article.select(\"a.Link-y81klt-0\"):\n",
    "                preproc = title.get_text()\n",
    "                preproc_ = preproc.split('\\n')[0]\n",
    "                i +=1\n",
    "\n",
    "                if i==2:\n",
    "                    #headline.append(preproc_)\n",
    "                    hd = preproc_\n",
    "\n",
    "            for time in article.select(\"time\"):\n",
    "                dt = time[\"datetime\"]\n",
    "                dt = datetime.datetime.strptime(dt, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\n",
    "                dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")           \n",
    "\n",
    "            nres = {\n",
    "                \"headline\" : hd,\n",
    "                \"date\" : dt_\n",
    "            }\n",
    "            collection.append(nres)\n",
    "            #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 32;\n                var nbb_unformatted_code = \"# for CNBC\\ndef get_cnbc(page):\\n    url = cnbc + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if(response.ok):\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"#pipeline_assetlist_0\\\")\\n\\n        for article in articles:\\n            headlines = article.select(\\\"div.headline > a\\\")\\n            timedata = article.select(\\\"time\\\")\\n        \\n        for i in range(0,len(headlines)):\\n            headline = str(headlines[i]).split('>')[1].split('<')[0].strip()\\n            date = str(timedata[i]).split('>')[1].split('<')[0].strip()\\n                    \\n            if \\\"Ago\\\" in date:\\n                parsed_s = [date.split()[:2]]\\n                parsed_s[0][1] = parsed_s[0][1].lower()\\n                if \\\"s\\\" not in parsed_s[0][1]:\\n                    parsed_s[0][1] += \\\"s\\\"\\n                time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\\n                dt = datetime.timedelta(**time_dict)\\n                past_time = datetime.datetime.now(est) - dt\\n                dt_ = past_time.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n                nres = {\\n                    \\\"headline\\\" : headline,\\n                    \\\"date\\\" : dt_\\n                }\\n                collection.append(nres)\\n\\n            elif \\\":\\\" in date:\\n                hours = date[:2].replace(':', '')\\n                day = date.split(' ')[-3]\\n\\n                if int(hours) < 10:\\n                    date = \\\"0\\\" + date                    \\n\\n                if 'Sept' or 'July' or 'June' or 'March' or 'April' in date: # March, April, June, July, and Sept do not come in three-letter abbreviated format\\n                    date = date.replace('Sept', 'Sep').replace('July', 'Jul').replace('June', 'Jun').replace('April', 'Apr').replace('March', 'Mar')\\n                dt = datetime.datetime.strptime(date, '%H:%M  %p ET %a,  %d %b %Y')\\n                dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n                nres = {\\n                    \\\"headline\\\" : headline,\\n                    \\\"date\\\" : dt_\\n                }\\n                collection.append(nres)\\n                #sleep(2)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n    \\n    return collection\";\n                var nbb_formatted_code = \"# for CNBC\\ndef get_cnbc(page):\\n    url = cnbc + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if response.ok:\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"#pipeline_assetlist_0\\\")\\n\\n        for article in articles:\\n            headlines = article.select(\\\"div.headline > a\\\")\\n            timedata = article.select(\\\"time\\\")\\n\\n        for i in range(0, len(headlines)):\\n            headline = str(headlines[i]).split(\\\">\\\")[1].split(\\\"<\\\")[0].strip()\\n            date = str(timedata[i]).split(\\\">\\\")[1].split(\\\"<\\\")[0].strip()\\n\\n            if \\\"Ago\\\" in date:\\n                parsed_s = [date.split()[:2]]\\n                parsed_s[0][1] = parsed_s[0][1].lower()\\n                if \\\"s\\\" not in parsed_s[0][1]:\\n                    parsed_s[0][1] += \\\"s\\\"\\n                time_dict = dict((fmt, float(amount)) for amount, fmt in parsed_s)\\n                dt = datetime.timedelta(**time_dict)\\n                past_time = datetime.datetime.now(est) - dt\\n                dt_ = past_time.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n                nres = {\\\"headline\\\": headline, \\\"date\\\": dt_}\\n                collection.append(nres)\\n\\n            elif \\\":\\\" in date:\\n                hours = date[:2].replace(\\\":\\\", \\\"\\\")\\n                day = date.split(\\\" \\\")[-3]\\n\\n                if int(hours) < 10:\\n                    date = \\\"0\\\" + date\\n\\n                if (\\n                    \\\"Sept\\\" or \\\"July\\\" or \\\"June\\\" or \\\"March\\\" or \\\"April\\\" in date\\n                ):  # March, April, June, July, and Sept do not come in three-letter abbreviated format\\n                    date = (\\n                        date.replace(\\\"Sept\\\", \\\"Sep\\\")\\n                        .replace(\\\"July\\\", \\\"Jul\\\")\\n                        .replace(\\\"June\\\", \\\"Jun\\\")\\n                        .replace(\\\"April\\\", \\\"Apr\\\")\\n                        .replace(\\\"March\\\", \\\"Mar\\\")\\n                    )\\n                dt = datetime.datetime.strptime(date, \\\"%H:%M  %p ET %a,  %d %b %Y\\\")\\n                dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n                nres = {\\\"headline\\\": headline, \\\"date\\\": dt_}\\n                collection.append(nres)\\n                # sleep(2)\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n\\n    return collection\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# for CNBC\n",
    "def get_cnbc(page):\n",
    "    url = cnbc + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"#pipeline_assetlist_0\")\n",
    "\n",
    "        for article in articles:\n",
    "            headlines = article.select(\"div.headline > a\")\n",
    "            timedata = article.select(\"time\")\n",
    "        \n",
    "        for i in range(0,len(headlines)):\n",
    "            headline = str(headlines[i]).split('>')[1].split('<')[0].strip()\n",
    "            date = str(timedata[i]).split('>')[1].split('<')[0].strip()\n",
    "                    \n",
    "            if \"Ago\" in date:\n",
    "                parsed_s = [date.split()[:2]]\n",
    "                parsed_s[0][1] = parsed_s[0][1].lower()\n",
    "                if \"s\" not in parsed_s[0][1]:\n",
    "                    parsed_s[0][1] += \"s\"\n",
    "                time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\n",
    "                dt = datetime.timedelta(**time_dict)\n",
    "                past_time = datetime.datetime.now(est) - dt\n",
    "                dt_ = past_time.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")\n",
    "\n",
    "                nres = {\n",
    "                    \"headline\" : headline,\n",
    "                    \"date\" : dt_\n",
    "                }\n",
    "                collection.append(nres)\n",
    "\n",
    "            elif \":\" in date:\n",
    "                hours = date[:2].replace(':', '')\n",
    "                day = date.split(' ')[-3]\n",
    "\n",
    "                if int(hours) < 10:\n",
    "                    date = \"0\" + date                    \n",
    "\n",
    "                if 'Sept' or 'July' or 'June' or 'March' or 'April' in date: # March, April, June, July, and Sept do not come in three-letter abbreviated format\n",
    "                    date = date.replace('Sept', 'Sep').replace('July', 'Jul').replace('June', 'Jun').replace('April', 'Apr').replace('March', 'Mar')\n",
    "                dt = datetime.datetime.strptime(date, '%H:%M  %p ET %a,  %d %b %Y')\n",
    "                dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")\n",
    "                nres = {\n",
    "                    \"headline\" : headline,\n",
    "                    \"date\" : dt_\n",
    "                }\n",
    "                collection.append(nres)\n",
    "                #sleep(2)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.investing.com/indices/us-spx-500-news/1\n",
      "here\n",
      "Mutual Funds Weekly: These money and investing tips can keep your portfolio on the cutting edge ['14 hours ago']\n",
      "here\n",
      "S&P 500, Dow Stumble, Dragged Down by Blue-Chip Technology Stalwarts IBM, Intel ['17 hours ago']\n",
      "here\n",
      "Tech shares could retake market reins as earnings heat up ['Jan 22, 2021']\n",
      "here\n",
      "Dow, S&P close lower as IBM, Intel weigh, coronavirus concerns rise ['Jan 22, 2021']\n",
      "here\n",
      "Dow Ends Week Higher Despite Slip on Energy, Tech Woes ['Jan 22, 2021']\n",
      "here\n",
      "Dow Slips on Hit From Tech, Energy ['Jan 22, 2021']\n",
      "here\n",
      "U.S. stocks mixed at close of trade; Dow Jones Industrial Average down 0.36% ['Jan 22, 2021']\n",
      "here\n",
      "Earnings Outlook: GE earnings: Can Q4 results, FCF guidance support the stock’s record rally? ['Jan 22, 2021']\n",
      "here\n",
      "S&P 500 Weekly Price Forecast – Stock Markets Continue to Look Bullish ['Jan 22, 2021']\n",
      "here\n",
      "S&P 500 Price Forecast – Stock Markets Continue to Show Demand ['Jan 22, 2021']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 28;\n                var nbb_unformatted_code = \"headers = { # bypass anti-scrapers, but I probably have to start using proxies\\n            \\\"User-Agent\\\":\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\\\"\\n}\\n\\nurl = inv + \\\"1\\\"\\nprint(url)\\nresponse = requests.get(url, headers=headers)\\ncollection = []\\n\\nif(response.ok):\\n    data = response.text\\n    soup = BeautifulSoup(data, \\\"html.parser\\\")\\n    articles = soup.select(\\\"div.mediumTitle1 > article.articleItem\\\")\\n\\n    for article in articles:\\n        text = article.get_text()\\n        text_ = text.strip().split('\\\\n')     \\n        to_filter = text_\\n\\n        while(\\\"\\\" in to_filter) : \\n            to_filter.remove(\\\"\\\")\\n        \\n        if (len(to_filter) > 1):\\n            print('here')\\n            headline = to_filter[0].strip()\\n            date = to_filter[1].strip()\\n            date_ = re.findall(\\\"\\\\w\\\\d\\\\sminute\\\\sago|\\\\w\\\\d\\\\sminutes\\\\sago|\\\\d\\\\shour\\\\sago|\\\\w\\\\d\\\\shours\\\\sago|\\\\d\\\\shours\\\\sago|Jan\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Feb\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Mar\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Apr\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|May\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jun\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jul\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Aug\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Sep\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Oct\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Nov\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Dec\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}\\\", date)\\n            first_char = date_[0][0] # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\\n\\n            print(headline, date_)\\n        \";\n                var nbb_formatted_code = \"headers = {  # bypass anti-scrapers, but I probably have to start using proxies\\n    \\\"User-Agent\\\": \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\\\"\\n}\\n\\nurl = inv + \\\"1\\\"\\nprint(url)\\nresponse = requests.get(url, headers=headers)\\ncollection = []\\n\\nif response.ok:\\n    data = response.text\\n    soup = BeautifulSoup(data, \\\"html.parser\\\")\\n    articles = soup.select(\\\"div.mediumTitle1 > article.articleItem\\\")\\n\\n    for article in articles:\\n        text = article.get_text()\\n        text_ = text.strip().split(\\\"\\\\n\\\")\\n        to_filter = text_\\n\\n        while \\\"\\\" in to_filter:\\n            to_filter.remove(\\\"\\\")\\n\\n        if len(to_filter) > 1:\\n            print(\\\"here\\\")\\n            headline = to_filter[0].strip()\\n            date = to_filter[1].strip()\\n            date_ = re.findall(\\n                \\\"\\\\w\\\\d\\\\sminute\\\\sago|\\\\w\\\\d\\\\sminutes\\\\sago|\\\\d\\\\shour\\\\sago|\\\\w\\\\d\\\\shours\\\\sago|\\\\d\\\\shours\\\\sago|Jan\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Feb\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Mar\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Apr\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|May\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jun\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jul\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Aug\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Sep\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Oct\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Nov\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Dec\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}\\\",\\n                date,\\n            )\\n            first_char = date_[0][\\n                0\\n            ]  # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\\n\\n            print(headline, date_)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "headers = { # bypass anti-scrapers, but I probably have to start using proxies\n",
    "            \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url = inv + \"1\"\n",
    "print(url)\n",
    "response = requests.get(url, headers=headers)\n",
    "collection = []\n",
    "\n",
    "if(response.ok):\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    articles = soup.select(\"div.mediumTitle1 > article.articleItem\")\n",
    "\n",
    "    for article in articles:\n",
    "        text = article.get_text()\n",
    "        text_ = text.strip().split('\\n')     \n",
    "        to_filter = text_\n",
    "\n",
    "        while(\"\" in to_filter) : \n",
    "            to_filter.remove(\"\")\n",
    "        \n",
    "        if (len(to_filter) > 1):\n",
    "            print('here')\n",
    "            headline = to_filter[0].strip()\n",
    "            date = to_filter[1].strip()\n",
    "            date_ = re.findall(\"\\w\\d\\sminute\\sago|\\w\\d\\sminutes\\sago|\\d\\shour\\sago|\\w\\d\\shours\\sago|\\d\\shours\\sago|Jan\\s+\\d{1,2},\\s+\\d{4}|Feb\\s+\\d{1,2},\\s+\\d{4}|Mar\\s+\\d{1,2},\\s+\\d{4}|Apr\\s+\\d{1,2},\\s+\\d{4}|May\\s+\\d{1,2},\\s+\\d{4}|Jun\\s+\\d{1,2},\\s+\\d{4}|Jul\\s+\\d{1,2},\\s+\\d{4}|Aug\\s+\\d{1,2},\\s+\\d{4}|Sep\\s+\\d{1,2},\\s+\\d{4}|Oct\\s+\\d{1,2},\\s+\\d{4}|Nov\\s+\\d{1,2},\\s+\\d{4}|Dec\\s+\\d{1,2},\\s+\\d{4}\", date)\n",
    "            first_char = date_[0][0] # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\n",
    "\n",
    "            print(headline, date_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 33;\n                var nbb_unformatted_code = \"# for Investing.com\\ndef get_investing(page):\\n    url = inv + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if(response.ok):\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"div.mediumTitle1 > article.articleItem\\\")\\n\\n        for article in articles:\\n            text = article.get_text()\\n            text_ = text.strip().split('\\\\n')\\n            \\n            to_filter = text_\\n\\n            while(\\\"\\\" in to_filter) : \\n                to_filter.remove(\\\"\\\")             \\n            \\n            if (len(to_filter) > 1):\\n                headline = to_filter[0].strip()\\n                date = to_filter[1].strip()\\n                date_ = re.findall(\\\"\\\\w\\\\d\\\\sminute\\\\sago|\\\\w\\\\d\\\\sminutes\\\\sago|\\\\d\\\\shour\\\\sago|\\\\w\\\\d\\\\shours\\\\sago|\\\\d\\\\shours\\\\sago|Jan\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Feb\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Mar\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Apr\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|May\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jun\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jul\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Aug\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Sep\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Oct\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Nov\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Dec\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}\\\", date)\\n                first_char = date_[0][0] # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\\n\\n                if(first_char.isdigit()):\\n                    parsed_s = [date_[0].split()[:2]]\\n                    if parsed_s[0][1] in [\\\"hour\\\", \\\"minute\\\"]:\\n                        parsed_s[0][1] += \\\"s\\\" # rare case but the new articles can be a minute or an hour old\\n                    time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\\n                    dt = datetime.timedelta(**time_dict)\\n                    past_time = datetime.datetime.now(est) - dt\\n                    dt_ = past_time.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n                    nres = {\\n                        \\\"headline\\\" : headline,\\n                        \\\"date\\\" : dt_\\n                    }\\n                    collection.append(nres)\\n                \\n                else:\\n                    dt = datetime.datetime.strptime(date_[0], '%b %d, %Y')\\n                    dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n                    nres = {\\n                        \\\"headline\\\" : headline,\\n                        \\\"date\\\" : dt_\\n                    }\\n                    collection.append(nres)\\n                    #sleep(2)\\n\\n        #print(collection)\\n        return collection\\n    \\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n        return collection\";\n                var nbb_formatted_code = \"# for Investing.com\\ndef get_investing(page):\\n    url = inv + str(page)\\n    print(url)\\n    response = requests.get(url, headers=headers)\\n    collection = []\\n\\n    if response.ok:\\n        data = response.text\\n        soup = BeautifulSoup(data, \\\"html.parser\\\")\\n        articles = soup.select(\\\"div.mediumTitle1 > article.articleItem\\\")\\n\\n        for article in articles:\\n            text = article.get_text()\\n            text_ = text.strip().split(\\\"\\\\n\\\")\\n\\n            to_filter = text_\\n\\n            while \\\"\\\" in to_filter:\\n                to_filter.remove(\\\"\\\")\\n\\n            if len(to_filter) > 1:\\n                headline = to_filter[0].strip()\\n                date = to_filter[1].strip()\\n                date_ = re.findall(\\n                    \\\"\\\\w\\\\d\\\\sminute\\\\sago|\\\\w\\\\d\\\\sminutes\\\\sago|\\\\d\\\\shour\\\\sago|\\\\w\\\\d\\\\shours\\\\sago|\\\\d\\\\shours\\\\sago|Jan\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Feb\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Mar\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Apr\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|May\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jun\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Jul\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Aug\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Sep\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Oct\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Nov\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}|Dec\\\\s+\\\\d{1,2},\\\\s+\\\\d{4}\\\",\\n                    date,\\n                )\\n                first_char = date_[0][\\n                    0\\n                ]  # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\\n\\n                if first_char.isdigit():\\n                    parsed_s = [date_[0].split()[:2]]\\n                    if parsed_s[0][1] in [\\\"hour\\\", \\\"minute\\\"]:\\n                        parsed_s[0][\\n                            1\\n                        ] += \\\"s\\\"  # rare case but the new articles can be a minute or an hour old\\n                    time_dict = dict((fmt, float(amount)) for amount, fmt in parsed_s)\\n                    dt = datetime.timedelta(**time_dict)\\n                    past_time = datetime.datetime.now(est) - dt\\n                    dt_ = past_time.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n\\n                    nres = {\\\"headline\\\": headline, \\\"date\\\": dt_}\\n                    collection.append(nres)\\n\\n                else:\\n                    dt = datetime.datetime.strptime(date_[0], \\\"%b %d, %Y\\\")\\n                    dt_ = dt.strftime(\\\"%Y-%m-%d %H:%M:%S-05:00\\\")\\n                    nres = {\\\"headline\\\": headline, \\\"date\\\": dt_}\\n                    collection.append(nres)\\n                    # sleep(2)\\n\\n        # print(collection)\\n        return collection\\n\\n    else:\\n        print(f\\\"No response on page {page}\\\")\\n        return collection\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# for Investing.com\n",
    "def get_investing(page):\n",
    "    url = inv + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"div.mediumTitle1 > article.articleItem\")\n",
    "\n",
    "        for article in articles:\n",
    "            text = article.get_text()\n",
    "            text_ = text.strip().split('\\n')\n",
    "            \n",
    "            to_filter = text_\n",
    "\n",
    "            while(\"\" in to_filter) : \n",
    "                to_filter.remove(\"\")             \n",
    "            \n",
    "            if (len(to_filter) > 1):\n",
    "                headline = to_filter[0].strip()\n",
    "                date = to_filter[1].strip()\n",
    "                date_ = re.findall(\"\\w\\d\\sminute\\sago|\\w\\d\\sminutes\\sago|\\d\\shour\\sago|\\w\\d\\shours\\sago|\\d\\shours\\sago|Jan\\s+\\d{1,2},\\s+\\d{4}|Feb\\s+\\d{1,2},\\s+\\d{4}|Mar\\s+\\d{1,2},\\s+\\d{4}|Apr\\s+\\d{1,2},\\s+\\d{4}|May\\s+\\d{1,2},\\s+\\d{4}|Jun\\s+\\d{1,2},\\s+\\d{4}|Jul\\s+\\d{1,2},\\s+\\d{4}|Aug\\s+\\d{1,2},\\s+\\d{4}|Sep\\s+\\d{1,2},\\s+\\d{4}|Oct\\s+\\d{1,2},\\s+\\d{4}|Nov\\s+\\d{1,2},\\s+\\d{4}|Dec\\s+\\d{1,2},\\s+\\d{4}\", date)\n",
    "                first_char = date_[0][0] # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\n",
    "\n",
    "                if(first_char.isdigit()):\n",
    "                    parsed_s = [date_[0].split()[:2]]\n",
    "                    if parsed_s[0][1] in [\"hour\", \"minute\"]:\n",
    "                        parsed_s[0][1] += \"s\" # rare case but the new articles can be a minute or an hour old\n",
    "                    time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\n",
    "                    dt = datetime.timedelta(**time_dict)\n",
    "                    past_time = datetime.datetime.now(est) - dt\n",
    "                    dt_ = past_time.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")\n",
    "\n",
    "                    nres = {\n",
    "                        \"headline\" : headline,\n",
    "                        \"date\" : dt_\n",
    "                    }\n",
    "                    collection.append(nres)\n",
    "                \n",
    "                else:\n",
    "                    dt = datetime.datetime.strptime(date_[0], '%b %d, %Y')\n",
    "                    dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S-05:00\")\n",
    "                    nres = {\n",
    "                        \"headline\" : headline,\n",
    "                        \"date\" : dt_\n",
    "                    }\n",
    "                    collection.append(nres)\n",
    "                    #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "        return collection\n",
    "    \n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 36;\n                var nbb_unformatted_code = \"# MAIN\\nfieldnames = ['headline', 'date']\\nheaders = { # bypass anti-scrapers, but I probably have to start using proxies\\n            \\\"User-Agent\\\":\\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\\\"\\n        }\\nest = timezone('EST') # all news are tagged for EST\\ncollection = [] # all news are to be put here\\nstart_p = int(input('Start Page: '))\\nend_p = int(input('End Page: '))\\n\\nfor p in range(start_p, end_p+1):\\n    collection.extend(get_dailyfx(p))\\n    collection.extend(get_fxempire(p))\\n    collection.extend(get_cnbc(p))\\n    collection.extend(get_investing(p))\\n    clear_output()\\n\\ntimenow = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H%MH -0800\\\")\\n\\npd.DataFrame(collection).to_csv(\\n    f'./data/news_{str(start_p)}-{str(end_p)}_{timenow}.zip', \\n    index=False, \\n    columns=['headline', 'date'], \\n    compression='zip'\\n    )\";\n                var nbb_formatted_code = \"# MAIN\\nfieldnames = [\\\"headline\\\", \\\"date\\\"]\\nheaders = {  # bypass anti-scrapers, but I probably have to start using proxies\\n    \\\"User-Agent\\\": \\\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\\\"\\n}\\nest = timezone(\\\"EST\\\")  # all news are tagged for EST\\ncollection = []  # all news are to be put here\\nstart_p = int(input(\\\"Start Page: \\\"))\\nend_p = int(input(\\\"End Page: \\\"))\\n\\nfor p in range(start_p, end_p + 1):\\n    collection.extend(get_dailyfx(p))\\n    collection.extend(get_fxempire(p))\\n    collection.extend(get_cnbc(p))\\n    collection.extend(get_investing(p))\\n    clear_output()\\n\\ntimenow = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H%MH -0800\\\")\\n\\npd.DataFrame(collection).to_csv(\\n    f\\\"./data/news_{str(start_p)}-{str(end_p)}_{timenow}.zip\\\",\\n    index=False,\\n    columns=[\\\"headline\\\", \\\"date\\\"],\\n    compression=\\\"zip\\\",\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# MAIN\n",
    "fieldnames = ['headline', 'date']\n",
    "headers = { # bypass anti-scrapers, but I probably have to start using proxies\n",
    "            \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"\n",
    "        }\n",
    "est = timezone('EST') # all news are tagged for EST\n",
    "collection = [] # all news are to be put here\n",
    "start_p = int(input('Start Page: '))\n",
    "end_p = int(input('End Page: '))\n",
    "\n",
    "for p in range(start_p, end_p+1):\n",
    "    collection.extend(get_dailyfx(p))\n",
    "    collection.extend(get_fxempire(p))\n",
    "    collection.extend(get_cnbc(p))\n",
    "    collection.extend(get_investing(p))\n",
    "    clear_output()\n",
    "\n",
    "timenow = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%MH -0800\")\n",
    "\n",
    "pd.DataFrame(collection).to_csv(\n",
    "    f'./data/news_{str(start_p)}-{str(end_p)}_{timenow}.zip', \n",
    "    index=False, \n",
    "    columns=['headline', 'date'], \n",
    "    compression='zip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}