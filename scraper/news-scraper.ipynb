{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('deeplearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c64c80a847a2bc1167ae9f3d93f268f3978450d0b767fc4cbce34d6ba3b096a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Scraper\n",
    "\n",
    "This scraper is built for this project that will determine whether the stock market prices will be on a down or uptrend based on news and stock price data. I still haven't determined whether I should include the price prediction. That depends well on the data I will be able to get after I have preprocessed the text from the news sources that I have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"%load_ext nb_black\";\n                var nbb_formatted_code = \"%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import clear_output\n",
    "from datetime import timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time, sleep\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import grequests\n",
    "import warnings\n",
    "import datetime\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "### Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = \"https://www.dailyfx.com/market-news/articles/\"\n",
    "fxe = \"https://www.fxempire.com/indices/spx500-usd/news?page=\"\n",
    "cnbc = \"https://www.cnbc.com/sp-500/?page=\"\n",
    "inv = \"https://www.investing.com/indices/us-spx-500-news/\"\n",
    "\n",
    "srcs = [dfx, fxe, cnbc, inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# checking the URLs of the news sites for a positive response\\nheaders = {\\n    \\\"User-Agent\\\":\\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\\\"\\n}\\n\\nrs = (grequests.get(u+\\\"1\\\", headers=headers) for u in srcs)\\ngrequests.map(rs)\";\n                var nbb_formatted_code = \"# checking the URLs of the news sites for a positive response\\nheaders = {\\n    \\\"User-Agent\\\": \\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\\\"\\n}\\n\\nrs = (grequests.get(u + \\\"1\\\", headers=headers) for u in srcs)\\ngrequests.map(rs)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# checking the URLs of the news sites for a positive response\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "}\n",
    "\n",
    "rs = (grequests.get(u+\"1\", headers=headers) for u in srcs)\n",
    "grequests.map(rs)"
   ]
  },
  {
   "source": [
    "Responses are good after setting a user agent. Might need to have a rotation of IP addresses since I intend to get around 50 pages' worth of headlines per source. Will resort to single thread pulls to avoid being tracked."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for DailyFX\n",
    "def get_dailyfx(page):\n",
    "    url = dfx + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"body > div.dfx-slidableContent > div > div.container > div > div.col-xl-8.dfx-border--r-xl-1 > div.dfx-articleList.jsdfx-articleList\")\n",
    "\n",
    "        for article in articles:  \n",
    "            headlines = article.select(\"span.align-middle\")\n",
    "            span = article.select(\"span.text-nowrap\")\n",
    "            \n",
    "        for i in range(0, len(headlines)):\n",
    "            headline = str(headlines[i]).split('>')[1].split('<')[0]\n",
    "            date = span[i]['data-time'].split('+')[0]\n",
    "            dt = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\n",
    "            dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S ET\")\n",
    "\n",
    "            nres = {\n",
    "                \"headline\" : headline,\n",
    "                \"date\" : dt_\n",
    "            }\n",
    "            collection.append(nres)\n",
    "            #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for FXEmpire\n",
    "def get_fxempire(page):\n",
    "    url = fxe + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"div.Article-sc-178sudu-0\")\n",
    "        \n",
    "        for article in articles:\n",
    "            text = article.get_text()\n",
    "            text_ = text.split('\\n')\n",
    "            i = 1\n",
    "\n",
    "            for title in article.select(\"a.Link-y81klt-0\"):\n",
    "                preproc = title.get_text()\n",
    "                preproc_ = preproc.split('\\n')[0]\n",
    "                i +=1\n",
    "\n",
    "                if i==2:\n",
    "                    #headline.append(preproc_)\n",
    "                    hd = preproc_\n",
    "\n",
    "            for time in article.select(\"time\"):\n",
    "                dt = time[\"datetime\"]\n",
    "                dt = datetime.datetime.strptime(dt, '%Y-%m-%dT%H:%M:%S') + timedelta(hours=-5) # adjusting for EST\n",
    "                dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S ET\")           \n",
    "\n",
    "            nres = {\n",
    "                \"headline\" : hd,\n",
    "                \"date\" : dt_\n",
    "            }\n",
    "            collection.append(nres)\n",
    "            #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNBC\n",
    "def get_cnbc(page):\n",
    "    url = cnbc + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"#pipeline_assetlist_0\")\n",
    "\n",
    "        for article in articles:\n",
    "            headlines = article.select(\"div.headline > a\")\n",
    "            timedata = article.select(\"time\")\n",
    "        \n",
    "        for i in range(0,len(headlines)):\n",
    "            headline = str(headlines[i]).split('>')[1].split('<')[0].strip()\n",
    "            date = str(timedata[i]).split('>')[1].split('<')[0].strip()\n",
    "                    \n",
    "            if \"Ago\" in date:\n",
    "                parsed_s = [date.split()[:2]]\n",
    "                parsed_s[0][1] = parsed_s[0][1].lower()\n",
    "                if \"s\" not in parsed_s[0][1]:\n",
    "                    parsed_s[0][1] += \"s\"\n",
    "                time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\n",
    "                dt = datetime.timedelta(**time_dict)\n",
    "                past_time = datetime.datetime.now(est) - dt\n",
    "                dt_ = past_time.strftime(\"%Y-%m-%d %H:%M:%S ET\")\n",
    "\n",
    "                nres = {\n",
    "                    \"headline\" : headline,\n",
    "                    \"date\" : dt_\n",
    "                }\n",
    "                collection.append(nres)\n",
    "\n",
    "            elif \":\" in date:\n",
    "                hours = date[:2].replace(':', '')\n",
    "                day = date.split(' ')[-3]\n",
    "\n",
    "                if int(hours) < 10:\n",
    "                    date = \"0\" + date                    \n",
    "\n",
    "                if 'Sept' or 'July' or 'June' or 'March' or 'April' in date: # March, April, June, July, and Sept do not come in three-letter abbreviated format\n",
    "                    date = date.replace('Sept', 'Sep').replace('July', 'Jul').replace('June', 'Jun').replace('April', 'Apr').replace('March', 'Mar')\n",
    "                dt = datetime.datetime.strptime(date, '%H:%M  %p ET %a,  %d %b %Y')\n",
    "                dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S ET\")\n",
    "                nres = {\n",
    "                    \"headline\" : headline,\n",
    "                    \"date\" : dt_\n",
    "                }\n",
    "                collection.append(nres)\n",
    "                #sleep(2)\n",
    "\n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Investing.com\n",
    "def get_investing(page):\n",
    "    url = inv + str(page)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    collection = []\n",
    "\n",
    "    if(response.ok):\n",
    "        data = response.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        articles = soup.select(\"div.mediumTitle1 > article.articleItem\")\n",
    "\n",
    "        for article in articles:\n",
    "            text = article.get_text()\n",
    "            text_ = text.strip().split('\\n')\n",
    "            \n",
    "            to_filter = text_\n",
    "\n",
    "            while(\"\" in to_filter) : \n",
    "                to_filter.remove(\"\")             \n",
    "            \n",
    "            if (len(to_filter) > 1):\n",
    "                headline = to_filter[0].strip()\n",
    "                date = to_filter[1].strip()\n",
    "                date_ = re.findall(\"\\w\\d\\sminute\\sago|\\w\\d\\sminutes\\sago|\\d\\shour\\sago|\\w\\d\\shours\\sago|\\d\\shours\\sago|Jan\\s+\\d{1,2},\\s+\\d{4}|Feb\\s+\\d{1,2},\\s+\\d{4}|Mar\\s+\\d{1,2},\\s+\\d{4}|Apr\\s+\\d{1,2},\\s+\\d{4}|May\\s+\\d{1,2},\\s+\\d{4}|Jun\\s+\\d{1,2},\\s+\\d{4}|Jul\\s+\\d{1,2},\\s+\\d{4}|Aug\\s+\\d{1,2},\\s+\\d{4}|Sep\\s+\\d{1,2},\\s+\\d{4}|Oct\\s+\\d{1,2},\\s+\\d{4}|Nov\\s+\\d{1,2},\\s+\\d{4}|Dec\\s+\\d{1,2},\\s+\\d{4}\", date)\n",
    "                first_char = date_[0][0] # this is important for backtracking a.k.a. knowing the approximate time stamp for the articles <1 day old\n",
    "\n",
    "                if(first_char.isdigit()):\n",
    "                    parsed_s = [date_[0].split()[:2]]\n",
    "                    if parsed_s[0][1] in [\"hour\", \"minute\"]:\n",
    "                        parsed_s[0][1] += \"s\" # rare case but the new articles can be a minute or an hour old\n",
    "                    time_dict = dict((fmt,float(amount)) for amount,fmt in parsed_s)\n",
    "                    dt = datetime.timedelta(**time_dict)\n",
    "                    past_time = datetime.datetime.now(est) - dt\n",
    "                    dt_ = past_time.strftime(\"%Y-%m-%d %H:%M:%S ET\")\n",
    "\n",
    "                    nres = {\n",
    "                        \"headline\" : headline,\n",
    "                        \"date\" : dt_\n",
    "                    }\n",
    "                    collection.append(nres)\n",
    "                \n",
    "                else:\n",
    "                    dt = datetime.datetime.strptime(date_[0], '%b %d, %Y')\n",
    "                    dt_ = dt.strftime(\"%Y-%m-%d %H:%M:%S ET\")\n",
    "                    nres = {\n",
    "                        \"headline\" : headline,\n",
    "                        \"date\" : dt_\n",
    "                    }\n",
    "                    collection.append(nres)\n",
    "                    #sleep(2)\n",
    "\n",
    "        #print(collection)\n",
    "        return collection\n",
    "    \n",
    "    else:\n",
    "        print(f\"No response on page {page}\")\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "fieldnames = ['headline', 'date']\n",
    "headers = { # bypass anti-scrapers, but I probably have to start using proxies\n",
    "            \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"\n",
    "        }\n",
    "est = timezone('EST') # all news are tagged for EST\n",
    "collection = [] # all news are to be put here\n",
    "start_p = int(input('Start Page: '))\n",
    "end_p = int(input('End Page: '))\n",
    "\n",
    "\n",
    "for p in range(start_p, end_p+1):\n",
    "    collection.extend(get_dailyfx(p))\n",
    "    collection.extend(get_fxempire(p))\n",
    "    collection.extend(get_cnbc(p))\n",
    "    collection.extend(get_investing(p))\n",
    "    clear_output()\n",
    "\n",
    "timenow = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%MH -0800\")\n",
    "\n",
    "pd.DataFrame(collection).to_csv(\n",
    "    f'./data/news/news_{str(start_p)}-{str(end_p)}_{timenow}.zip', \n",
    "    index=False, \n",
    "    columns=['headline', 'date'], \n",
    "    compression='zip'\n",
    "    )"
   ]
  }
 ]
}